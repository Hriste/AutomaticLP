{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Christina Paolicelli\n",
    "3/1/20\n",
    "\n",
    "LP Object Detection\n",
    "'''\n",
    "\n",
    "'''\n",
    "Pretrained Model from here: https://heartbeat.fritz.ai/detecting-objects-in-videos-and-camera-feeds-using-keras-opencv-and-imageai-c869fe1ebcdb\n",
    "'''\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "from object_detection.utils import dataset_util\n",
    "from object_detection.utils import label_map_util\n",
    "import pathlib\n",
    "import hashlib\n",
    "\n",
    "TRAINPATH = \"../GeneratedImages_2020-03-01_15-54\"\n",
    "EVALPATH = \"../GeneratedImages_2020-03-01_19-22\"\n",
    "CLASS_NAMES = ['0','1','2','3','4','5','6','7','8','9','A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'R', 'S', 'T', 'V', 'W', 'X', 'Y', 'Z']\n",
    "PATH_TO_LABELS = './data/label_map.pbtxt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTFRecord(row, imgPath):\n",
    "    height = 256\n",
    "    width = 512\n",
    "    image_format = 'png'.encode('utf-8')\n",
    "    \n",
    "    xmins = [] # List of normalized left x coordinates in bounding box (1 per box)\n",
    "    xmaxs = [] # List of normalized right x coordinates in bounding box\n",
    "             # (1 per box)\n",
    "    ymins = [] # List of normalized top y coordinates in bounding box (1 per box)\n",
    "    ymaxs = [] # List of normalized bottom y coordinates in bounding box\n",
    "             # (1 per box)\n",
    "    classes_text = [] # List of string class name of bounding box (1 per box)\n",
    "    classes = [] # List of integer class id of bounding box (1 per box)\n",
    "\n",
    "\n",
    "    # We know there are 7 characters in the LP\n",
    "    filename = row[0].encode('utf-8')\n",
    "    # 1st Character\n",
    "    classes_text.append(CLASS_NAMES[int(row[1])].encode('utf-8'))\n",
    "    classes.append(int(row[1]))\n",
    "    xmins.append(float(row[2])/width)\n",
    "    ymins.append(float(row[3])/height)\n",
    "    xmaxs.append(float(row[4])/width)\n",
    "    ymaxs.append(float(row[5])/height)\n",
    "    # 2nd Character\n",
    "    classes_text.append(CLASS_NAMES[int(row[6])].encode('utf-8'))\n",
    "    classes.append(int(row[6]))\n",
    "    xmins.append(float(row[7])/width)\n",
    "    ymins.append(float(row[8])/height)\n",
    "    xmaxs.append(float(row[9])/width)\n",
    "    ymaxs.append(float(row[10])/height)\n",
    "    # 3rd Character\n",
    "    classes_text.append(CLASS_NAMES[int(row[11])].encode('utf-8'))\n",
    "    classes.append(int(row[11]))\n",
    "    xmins.append(float(row[12])/width)\n",
    "    ymins.append(float(row[13])/height)\n",
    "    xmaxs.append(float(row[14])/width)\n",
    "    ymaxs.append(float(row[15])/height)\n",
    "    # 4th Character\n",
    "    classes_text.append(CLASS_NAMES[int(row[16])].encode('utf-8'))\n",
    "    classes.append(int(row[16]))\n",
    "    xmins.append(float(row[17])/width)\n",
    "    ymins.append(float(row[18])/height)\n",
    "    xmaxs.append(float(row[19])/width)\n",
    "    ymaxs.append(float(row[20])/height)\n",
    "    # 5th Character\n",
    "    classes_text.append(CLASS_NAMES[int(row[21])].encode('utf-8'))\n",
    "    classes.append(int(row[21]))\n",
    "    xmins.append(float(row[22])/width)\n",
    "    ymins.append(float(row[23])/height)\n",
    "    xmaxs.append(float(row[24])/width)\n",
    "    ymaxs.append(float(row[25])/height)\n",
    "    # 6th Character\n",
    "    classes_text.append(CLASS_NAMES[int(row[26])].encode('utf-8'))\n",
    "    classes.append(int(row[26]))\n",
    "    xmins.append(float(row[27])/width)\n",
    "    ymins.append(float(row[28])/height)\n",
    "    xmaxs.append(float(row[29])/width)\n",
    "    ymaxs.append(float(row[30])/height)\n",
    "    # 7th Character\n",
    "    classes_text.append(CLASS_NAMES[int(row[31])].encode('utf-8'))\n",
    "    classes.append(int(row[31]))\n",
    "    xmins.append(float(row[32])/width)\n",
    "    ymins.append(float(row[33])/height)\n",
    "    xmaxs.append(float(row[34])/width)\n",
    "    ymaxs.append(float(row[35])/height)\n",
    "\n",
    "    \n",
    "    img_path = os.path.join(imgPath, row[0])\n",
    "    with tf.gfile.GFile(img_path, 'rb') as fid:\n",
    "        encoded_jpg = fid.read()\n",
    "    \n",
    "    key = hashlib.sha256(encoded_jpg).hexdigest()\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "      'image/height': dataset_util.int64_feature(height),\n",
    "      'image/width': dataset_util.int64_feature(width),\n",
    "      'image/filename': dataset_util.bytes_feature(filename),\n",
    "      'image/source_id': dataset_util.bytes_feature(filename),\n",
    "      'image/format': dataset_util.bytes_feature(image_format),\n",
    "      'image/key/sha256':dataset_util.bytes_feature(key.encode('utf8')),\n",
    "      'image/encoded':dataset_util.bytes_feature(encoded_jpg),\n",
    "      'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "      'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "      'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "      'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "      'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "      'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "      }))\n",
    "    return tf_example\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    base_url = 'http://download.tensorflow.org/models/object_detection/'\n",
    "    model_file = model_name + '.tar.gz'\n",
    "    model_dir = tf.keras.utils.get_file(\n",
    "      fname=model_name, \n",
    "      origin=base_url + model_file,\n",
    "      untar=True)\n",
    "\n",
    "    model_dir = pathlib.Path(model_dir)/\"saved_model\"\n",
    "\n",
    "    model = tf.saved_model.load(str(model_dir))\n",
    "    model = model.signatures['serving_default']\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_(serialized_example):\n",
    "    feature = {'image_raw':tf.FixedLenFeature([],tf.string),\n",
    "                'label':tf.FixedLenFeature([],tf.int64)}\n",
    "    example = tf.parse_single_example(serialized_example,feature)\n",
    "    image = tf.decode_raw(example['image_raw'],tf.int64) #remember to parse in int64. float will raise error\n",
    "    label = tf.cast(example['label'],tf.int32)\n",
    "    return (dict({'image':image}),label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfrecord_train_input_fn(batch_size=32):\n",
    "    tfrecord_dataset = tf.data.TFRecordDataset(\"TFRecord.tfrecord\")\n",
    "    tfrecord_dataset = tfrecord_dataset.map(lambda   x:_parse_(x)).shuffle(True).batch(batch_size)\n",
    "    tfrecord_iterator = tfrecord_dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return tfrecord_iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Generate TF Record for Testing\n",
    "writer = tf.io.TFRecordWriter(\"TFRecordTrain.tfrecord\")\n",
    "    \n",
    "with open(TRAINPATH+\"/dataset.csv\") as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    next(csvreader)\n",
    "    for row in csvreader:\n",
    "        tf_example = createTFRecord(row, TRAINPATH)\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "        \n",
    "writer.close()\n",
    "\n",
    "# Generate TF Record for Evaluation\n",
    "writer = tf.io.TFRecordWriter(\"TFRecordEval.tfrecord\")\n",
    "    \n",
    "with open(EVALPATH+\"/dataset.csv\") as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    next(csvreader)\n",
    "    for row in csvreader:\n",
    "        tf_example = createTFRecord(row, EVALPATH)\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "        \n",
    "writer.close()\n",
    "print('Done')\n",
    "#model_name = 'ssd_mobilenet_v1_coco_2018_01_28'\n",
    "#detection_model = load_model(model_name)\n",
    "\n",
    "# https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_locally.md\n",
    "#old_dir = os.getcwd()\n",
    "#os.chdir('home/christina/Documents/Thesis/models/research')\n",
    "#PIPELINE_CONFIG_PATH = os.join(old_dir, 'models/model/ssd_mobilenet_v1_coco.config')\n",
    "#MODEL_DIR=\"models/model\"\n",
    "#NUM_TRAIN_STEPS = 100 # the tutorial had 50000\n",
    "#SAMPLE_1_OF_N_EVAL_EXAMPLES=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
